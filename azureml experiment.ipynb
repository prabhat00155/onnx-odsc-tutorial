{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureML experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully trained a model and validated the predictions are as expected, we will want to prepare this for use in a production environment. To operationalize this there are many more considerations beyond just training and inferencing the model; we must consider the compute for training/retraining, distributed training, model management, containers for deployment, and other factors. \n",
    "\n",
    "**Azure Machine Learning service** is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.\n",
    "\n",
    "![AML Overview](./images/aml-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Concepts\n",
    "\n",
    "The machine learning model workflow generally follows this sequence:\n",
    "\n",
    "1. Train\n",
    "    * Develop machine learning training scripts in **Python** or with the visual designer.\n",
    "    * Create and configure a **compute target**.\n",
    "    * Submit the **scripts** to the configured compute target to run in that environment. During training, the scripts can read from or write to datastore. And the records of execution are saved as runs in the workspace and grouped under **experiments**.\n",
    "\n",
    "2. Package \n",
    "    * After a satisfactory run is found, register the persisted model in the **model registry**.\n",
    "\n",
    "3. Validate\n",
    "    * **Query the experiment** for logged metrics from the current and past runs. If the metrics don't indicate a desired outcome, loop back to step 1 and iterate on your scripts.\n",
    "\n",
    "4. Deploy \n",
    "    * Develop a **scoring script** that uses the model and Deploy the model as a **web service** in Azure, or to an IoT Edge device.\n",
    "\n",
    "5. Monitor\n",
    "    * Monitor for data drift between the training dataset and inference data of a deployed model. When necessary, loop back to step 1 to retrain the model with new training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Get Started\n",
    "\n",
    "**First, let's make sure we have the SDK installed and check the version.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK. The latest version is 1.0.74.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to your AML Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. The workspace holds all your experiments, compute targets, models, datastores, etc.\n",
    "\n",
    "You can open [ml.azure.com](https://ml.azure.com/) to access your workspace resources through a graphical user interface of Azure Machine Learning studio.\n",
    "\n",
    "![AML workspace](./images/aml-workspace.png)\n",
    "\n",
    "**You will be asked to login in the next step.** Use the credentials you used to sign in to Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've already created a workspace, we can load it now. Import the Workspace class, and load your subscription information from the file config.json using the function from_config(). This looks for the JSON file in the current directory by default, but you can also specify a path parameter to point to the file using from_config(path=\"your/file/path\"). **In a cloud notebook server, the file is automatically in the root directory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a remote compute target\n",
    "\n",
    "A [compute target](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.computetarget?view=azure-ml-py) is a designated compute resource/environment where you run your training script or host your service deployment. This location may be your local machine or a cloud-based compute resource. Compute targets can be reused across the workspace for different runs and experiments.\n",
    "\n",
    "In this tutorial, we will use the General Purpose D3_v2 VM as your training compute resource. (more details on compute target options [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-compute-target)) This code creates a cluster for you if it does not already exist in your workspace.\n",
    "\n",
    "Creation of the cluster takes approximately 5 minutes. If the cluster is already in your workspace this code will skip the cluster creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2', \n",
    "                                                           max_nodes=6)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current cluster. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory to hold all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script and any additional files your training script depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './pytorch-exp'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write the training script we went through in the first part of the workshop to a .py file - [PyTorch experiment](https://github.com/prabhat00155/onnx-odsc-tutorial/blob/master/pytorch%20experiment.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import (\n",
    "    Variable,\n",
    ")\n",
    "from torch.nn import (\n",
    "    init,\n",
    ")\n",
    "from torchvision import (\n",
    "    datasets, \n",
    "    transforms,\n",
    "    models,\n",
    "    utils,\n",
    ")\n",
    "\n",
    "model_name = 'resnet18'\n",
    "num_workers = 2\n",
    "num_epochs = 2 \n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "dropout_p = 0.4\n",
    "decay_rate = 0.9999\n",
    "max_grad_norm = 5.0\n",
    "log_interval = 1\n",
    "num_classes = 8\n",
    "\n",
    "# reproduceability\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Fetch the dataset(in rar format), unrar it after install unrar. \n",
    "os.system('wget https://www.rarlab.com/rar/rarlinux-x64-5.6.0.tar.gz')\n",
    "os.system('tar -zxvf rarlinux-x64-5.6.0.tar.gz')\n",
    "os.system('./rar/unrar')\n",
    "os.system(\n",
    "'wget http://vision.stanford.edu/lijiali/event_dataset/event_dataset.rar')\n",
    "os.system('./rar/unrar x event_dataset.rar')\n",
    "\n",
    "# Load the data, split it among train, test and validation set after applying a series of transforms.\n",
    "image_folder = 'event_img/'\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.47637546, 0.485785  , 0.4522678 ], [0.24692202, 0.24377407, 0.2667196 ])\n",
    "    ])\n",
    "data = datasets.ImageFolder(root=image_folder, transform=data_transforms)\n",
    "class_names = data.classes\n",
    "train_len, val_len = int(len(data) * 0.75), int(len(data) * 0.2)\n",
    "test_len = len(data) - train_len - val_len\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(data, [train_len, val_len, test_len])\n",
    "loader = {\n",
    "    'train': torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
    "    'test': torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
    "    'val': torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "}\n",
    "\n",
    "# Pick one of the pre-trained models, replace its final layer setting its output to the number of classes.\n",
    "model = models.__dict__[model_name](pretrained=True) # Set false to train from scratch\n",
    "# Alter the final layer\n",
    "final_layer_input = model.fc.in_features\n",
    "# nn.Linear a linear transformation to the incoming data: y = x A^T + b\n",
    "model.fc = nn.Linear(final_layer_input, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=learning_rate, \n",
    "                      momentum=momentum, \n",
    "                      weight_decay=weight_decay,\n",
    "                     )\n",
    "\n",
    "\n",
    "def process_batch(inputs, targets, model, criterion, optimizer, is_training):\n",
    "    \"\"\"\n",
    "    Process a minibatch for loss and accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert tensors to Variables (for autograd)\n",
    "    if is_training:\n",
    "        X_batch = Variable(inputs, requires_grad=False)\n",
    "    else:\n",
    "        X_batch = Variable(inputs, volatile=True, requires_grad=False)\n",
    "    y_batch = Variable(targets.long(), requires_grad=False)\n",
    "\n",
    "    # Forward pass\n",
    "    scores = model(X_batch) # logits\n",
    "\n",
    "    # Loss\n",
    "    loss = criterion(scores, y_batch)\n",
    "    \n",
    "    # Accuracy\n",
    "    score, predicted = torch.max(scores, 1)\n",
    "    accuracy = (y_batch.data == predicted.data).sum() / float(len(y_batch))\n",
    "    \n",
    "    if is_training:\n",
    "\n",
    "        # In PyTorch, we need to set the gradients to zero before starting to\n",
    "        # do backpropragation because PyTorch accumulates the gradients on subsequent\n",
    "        # backward passes.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradient norms\n",
    "        nn.utils.clip_grad_norm(model.parameters(), max_grad_norm)\n",
    "\n",
    "        # Update params\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, test_loader, \n",
    "          num_epochs, batch_size, log_interval, learning_rate,\n",
    "          dropout_p, decay_rate, max_grad_norm):\n",
    "    \"\"\"\n",
    "    Training the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Metrics\n",
    "    train_loss, train_acc = [], []\n",
    "    test_loss, test_acc = [], []\n",
    "\n",
    "    # Training\n",
    "    for num_train_epoch in range(num_epochs):\n",
    "        # Timer\n",
    "        start = time.time()\n",
    "\n",
    "        # Decay learning rate\n",
    "        learning_rate = learning_rate * (decay_rate ** (num_train_epoch // 1.0))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "        # Metrics\n",
    "        train_batch_loss, train_batch_accuracy = 0.0, 0.0\n",
    "\n",
    "        for train_batch_num, (inputs, target) in enumerate(train_loader):\n",
    "            # Get metrics\n",
    "            model.train()\n",
    "            loss, accuracy = process_batch(\n",
    "                inputs, target, model, criterion, optimizer, model.training)\n",
    "            \n",
    "            # Add to batch scalars\n",
    "            train_batch_loss += loss.data.item() / float(len(inputs))\n",
    "            train_batch_accuracy += accuracy\n",
    "            \n",
    "        # Add to global metrics\n",
    "        train_loss.append(train_batch_loss / float(train_batch_num+1))\n",
    "        train_acc.append(train_batch_accuracy / float(train_batch_num+1))\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        for num_test_epoch in range(1):\n",
    "            # Metrics\n",
    "            test_batch_loss, test_batch_accuracy = 0.0, 0.0\n",
    "\n",
    "            for test_batch_num, (inputs, target) in enumerate(test_loader):\n",
    "                # Get metrics\n",
    "                model.eval()\n",
    "                loss, accuracy = process_batch(\n",
    "                    inputs, target, model, criterion, optimizer, model.training)\n",
    "                # Add to batch scalars\n",
    "                test_batch_loss += loss.data.item() / float(len(inputs))\n",
    "                test_batch_accuracy += accuracy\n",
    "\n",
    "            # Add to global metrics\n",
    "            test_loss.append(test_batch_loss / float(test_batch_num+1))\n",
    "            test_acc.append(test_batch_accuracy / float(test_batch_num+1))\n",
    "                \n",
    "\n",
    "            verbose_condition = ((num_train_epoch == 0) or (num_train_epoch % log_interval == 0) \n",
    "                                 or (num_train_epoch == num_epochs-1))\n",
    "\n",
    "            # Verbose\n",
    "            if verbose_condition:\n",
    "                time_remain = (time.time() - start) * (num_epochs - (num_train_epoch + 1))\n",
    "                minutes = time_remain // 60\n",
    "                seconds = time_remain - minutes * 60\n",
    "                print(f'TIME REMAINING: {minutes:.0f}m {seconds:.0f}s')\n",
    "                print(f'[EPOCH]: {num_train_epoch},'\n",
    "                      f'[TRAIN LOSS]: {train_batch_loss / float(train_batch_num+1):.6f},'\n",
    "                      f'[TRAIN ACC]: {train_batch_accuracy / float(train_batch_num+1):.3f},'\n",
    "                      f'[VAL LOSS]: {test_batch_loss / float(test_batch_num+1):.6f},'\n",
    "                      f'[VAL ACC]: {test_batch_accuracy / float(test_batch_num+1):.3f}')\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train(model, criterion, optimizer, loader['train'], loader['val'], \n",
    "              num_epochs, batch_size, log_interval, learning_rate,\n",
    "              dropout_p, decay_rate, max_grad_norm)\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='PyTorch Sports Image Classification')\n",
    "parser.add_argument('--output-dir', type=str, default='outputs')\n",
    "args = parser.parse_args()\n",
    "output_dir = args.output_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "model_path = os.path.join(output_dir, 'sports_classification-1.onnx')\n",
    "torch.onnx.export(model, dummy_input, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mv train.py pytorch-exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an Experiment to track all the runs in your workspace for this transfer learning PyTorch tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch1-sports'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorch estimator\n",
    "The AML SDK's PyTorch estimator enables you to easily submit PyTorch training jobs for both single-node and distributed runs. For more information on the PyTorch estimator, refer here(https://docs.microsoft.com/en-gb/azure/machine-learning/service/how-to-train-pytorch). The following code will define a single-node PyTorch job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "estimator = PyTorch(source_directory=project_folder, \n",
    "                    script_params={'--output-dir': './outputs'},\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script='train.py',\n",
    "                    use_gpu=False)\n",
    "\n",
    "# upgrade to the latest version of PyTorch, which has better support for ONNX\n",
    "estimator.conda_dependencies.remove_conda_package('pytorch=0.4.0')\n",
    "estimator.conda_dependencies.add_conda_package('pytorch')\n",
    "estimator.conda_dependencies.add_channel('pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script_params parameter is a dictionary containing the command-line arguments to your training script entry_script. Please note the following:\n",
    "We specified the output directory as ./outputs. The outputs directory is specially treated by AML in that all the content in this directory gets uploaded to your workspace as part of your run history. The files written to this directory are therefore accessible even once your remote run is over. In this tutorial, we will save our trained model to this output directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job\n",
    "Run your experiment by submitting your estimator object. Note that this call is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(estimator)\n",
    "print(run.get_details())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your run\n",
    "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can block until the script has completed training before running more code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model (optional)\n",
    "Once the run completes, you can choose to download the ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the files from the run\n",
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('outputs', 'sports_classification-1.onnx')\n",
    "run.download_file(model_path, output_file_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also view a visualization of the model using [Netron](https://lutzroeder.github.io/netron/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model\n",
    "To keep track of our models from various runs we may be testing, we will register the model from the run to our workspace. The model_path parameter takes in the relative path on the remote VM to the model file in your outputs directory. You can then deploy this registered model as a web service through the AML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='sports_classification-1', model_path=model_path)\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying your registered models (optional)\n",
    "To see all the models you've registered, you can list them as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ws.models\n",
    "for name, m in models.items():\n",
    "    print(\"Name:\", name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying as a web service\n",
    "Now we are ready to [deploy](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where) the model as a web service. For this notebook we will deploy this to run on an Azure Container Instance [ACI](https://azure.microsoft.com/en-us/services/container-instances/), but you can alternatively also run on your [local](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#local) machine or with Azure Kubernetes Service [AKS](https://azure.microsoft.com/en-us/services/kubernetes-service/). \n",
    "\n",
    "Azure Machine Learning accomplishes this by constructing a Docker image with the scoring logic and model baked in. We will deploy our ONNX model on Azure ML using ONNX Runtime inference engine. \n",
    "\n",
    "**To build the correct environment, provide the following:**\n",
    "\n",
    "* A scoring script to show how to use the model\n",
    "* An environment file to show what packages need to be installed\n",
    "* A configuration file to build the web service\n",
    "* The model you trained before\n",
    "\n",
    "### Write scoring file\n",
    "We begin by writing a score.py file that will be invoked by the web service call.\n",
    "\n",
    "Note that the scoring script must have two required functions, init() and run(input_data).\n",
    "* The **init()** function is called once when the container is started so we load the model using the ONNX Runtime into a global session object. This function is executed only once when the Docker container is started.\n",
    "* In **run(input_data)** function, the model is used to predict a value based on the input data. The input and output to run typically use JSON as serialization and de-serialization format but you are not limited to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from azureml.core.model import Model\n",
    "import numpy as np\n",
    "from onnxruntime import InferenceSession\n",
    "from torchvision import transforms\n",
    "\n",
    "def init():\n",
    "    global session\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_onnx = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sports_classification-1.onnx')\n",
    "    session = InferenceSession(model_onnx)\n",
    "\n",
    "def preprocess(input_data_json):\n",
    "    input_url = json.loads(input_data_json)['data'][0]\n",
    "    # convert the image url into the tensor input\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.47637546, 0.485785  , 0.4522678 ], [0.24692202, 0.24377407, 0.2667196 ])\n",
    "    ])  \n",
    "    response = requests.get(input_url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    image = data_transforms(image)\n",
    "    image = image.numpy().reshape((1, *image.shape))\n",
    "    return image\n",
    "\n",
    "def postprocess(result):\n",
    "    class_names = [ \n",
    "        'RockClimbing',\n",
    "        'badminton',\n",
    "        'bocce',\n",
    "        'croquet',\n",
    "        'polo',\n",
    "        'rowing',\n",
    "        'sailing',\n",
    "        'snowboarding'\n",
    "    ]   \n",
    "    return class_names[np.argmax(result[0])]\n",
    "\n",
    "def run(input_data_json):\n",
    "    try:\n",
    "        start = time.time()   # start timer\n",
    "        input_data = preprocess(input_data_json)\n",
    "        input_name = session.get_inputs()[0].name  # get the id of the first input of the model   \n",
    "        result = session.run(None, {input_name: input_data})\n",
    "        end = time.time()     # stop timer\n",
    "        return {\"result\": postprocess(result),\n",
    "                \"time\": end - start}\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return {\"error\": result}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inference configuration\n",
    "First we create a YAML file that specifies which dependencies we would like to see in our container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(pip_packages=[\"numpy\",\"onnxruntime\",\"azureml-core\", \"Pillow\", \"torchvision\"])\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we setup the [inference configuration](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(runtime= \"python\", \n",
    "                                   entry_script=\"score.py\",\n",
    "                                   conda_file=\"myenv.yml\",\n",
    "                                   extra_docker_file_steps = \"Dockerfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model using [Azure Container Instances](https://docs.microsoft.com/en-us/azure/container-instances/container-instances-overview)\n",
    "**Estimated time to complete: about 3-7 minutes**\n",
    "\n",
    "Configure the image and deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 1, \n",
    "                                               tags = {'demo': 'onnx'}, \n",
    "                                               description = 'web service for sports classification ONNX model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code goes through these steps:\n",
    "\n",
    "Build an image using:\n",
    "* The scoring file (score.py)\n",
    "* The environment file (myenv.yml)\n",
    "* The model file\n",
    "* Define ACI Deployment Configuration\n",
    "* Send the image to the ACI container.\n",
    "* Start up a container in ACI using the image.\n",
    "* Get the web service HTTP endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "from random import randint\n",
    "\n",
    "aci_service_name = f'onnx-sports{randint(0,100)}'\n",
    "print(\"Service\", aci_service_name)\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the deployment fails, you can check the logs. Make sure to delete your aci_service before trying again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aci_service.state != 'Healthy':\n",
    "    # run this command for debugging.\n",
    "    print(aci_service.get_logs())\n",
    "    aci_service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success!\n",
    "If you've made it this far, you've deployed a working web service that does sports image classification using an ONNX model. You can get the URL for the webservice with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_url = aci_service.scoring_uri\n",
    "service_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the service\n",
    "To submit sample data to the running service, use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "image_name = 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/30/DN_ice_boat--Ice_Nine--Lake_Sunapee_NH.jpg/220px-DN_ice_boat--Ice_Nine--Lake_Sunapee_NH.jpg'\n",
    "Image(url= image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "test_sample = json.dumps({'data': [\n",
    "    image_name\n",
    "]})\n",
    "test_sample = bytes(test_sample,encoding = 'utf8')\n",
    "headers = {'Content-Type':'application/json'}\n",
    "resp = requests.post(service_url, test_sample, headers=headers)\n",
    "print(\"prediction:\", resp.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
